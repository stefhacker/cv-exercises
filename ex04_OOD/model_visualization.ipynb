{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e96108-bfa6-4f28-a3f4-3002d809b829",
   "metadata": {},
   "source": [
    "# Load and visualize models\n",
    "\n",
    "Install prerequisites:\n",
    "\n",
    "```bash\n",
    "# huggingface transformers\n",
    "pip install transformers\n",
    "\n",
    "# visualizer\n",
    "conda install -y graphviz python-graphviz\n",
    "pip install torchlens\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62572d-87c2-4ba0-a65a-8b40f3e34c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision import models\n",
    "\n",
    "from torchview import draw_graph\n",
    "import torchlens as tl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c23bd-5cd0-42ab-846f-3d1654282b92",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d6745-5537-49d9-9dbb-7f3158425341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: models.resnet.ResNet = models.resnet18(\n",
    "    weights=ResNet18_Weights.IMAGENET1K_V1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612691e5-cdf4-473c-a403-694248b02379",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tl.log_forward_pass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c63b53-9119-46bd-8f83-ebca609b4867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history  = tl.log_forward_pass(\n",
    "    model, torch.zeros((2, 3, 224, 224)),\n",
    "    vis_opt=\"rolled\",\n",
    "    vis_direction=\"topdown\",\n",
    "    vis_fileformat=\"svg\",\n",
    "    vis_outpath=\"resnet18.svg\",\n",
    ")\n",
    "print(model_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f7b17-ae4a-4520-94d7-079856dc8dea",
   "metadata": {},
   "source": [
    "## VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633717f-d5a7-4719-bdd8-2fbb8ddb1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"WinKawaks/vit-small-patch16-224\")\n",
    "vit_model = AutoModelForImageClassification.from_pretrained(\"WinKawaks/vit-small-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa75f6d-0986-473f-8f6a-ef8b17838064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in vit_model.config.to_dict().items():\n",
    "    if k in [\"label2id\", \"id2label\"]:\n",
    "        print(f\"{k:40s}: length {len(v)}\")\n",
    "        continue\n",
    "    print(f\"{k:40s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3de6a2-a6a3-49d2-ad9a-1ef4b59d696d",
   "metadata": {},
   "source": [
    "Inspect the model code to find out where the positional encoding is added, since that is not obvious from the model graph output.\n",
    "\n",
    "Turns position encodings are learned weights that are added at the end of `vit_model.vit.embeddings.forward`\n",
    "\n",
    "In the graph they show up as `add_1_6 params 1x197x384`:\n",
    "\n",
    "Sequence length is 197 = 1 Learned \"CLS\" token + 14x14 positional embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd8513-fd88-49c2-8492-861ae6f4a178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(vit_model.forward))\n",
    "print(\"========================================================\")\n",
    "print(inspect.getsource(vit_model.vit.forward))\n",
    "print(\"========================================================\")\n",
    "print(inspect.getsource(vit_model.vit.embeddings.forward))\n",
    "print(\"========================================================\")\n",
    "print(vit_model.vit.embeddings.position_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b8377-e474-4242-a375-330f29aab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5768f-1743-4f89-b714-afbec8a7dd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history  = tl.log_forward_pass(\n",
    "    vit_model, torch.zeros((2, 3, 224, 224)),\n",
    "    vis_opt=\"unrolled\",\n",
    "    vis_direction=\"topdown\",\n",
    "    vis_fileformat=\"svg\",\n",
    "    vis_outpath=\"vit_small.svg\",\n",
    "    vis_nesting_depth=99,\n",
    ")\n",
    "print(model_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c7eb6-b2c6-4848-ac16-a6e690bac048",
   "metadata": {},
   "source": [
    "## Generative Language Transformer\n",
    "\n",
    "With one forward pass we can generate one token (\"word\") at a time.\n",
    "\n",
    "Here: \"Hello my name\" -> model -> \" is\"\n",
    "\n",
    "For a longer output you would loop. Huggingface allows this functionality with `model.generate`\n",
    "\n",
    "Notes: Importantly for a batch of text with varying input length you would need to properly pad (pad left side for generative transformers) and pass attention_mask to mask the padding tokens.\n",
    "\n",
    "\n",
    "In the graph you can see the embedding consists of a language embedding (50257x768) which maps from tokens to embeddings (the tokenizer before mapped text to tokens) and a position embedding (1024x768) from which only the first 3 positions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089996e-5949-4e12-af9a-aca6ef9cfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "tokens = tokenizer(\"Hello my name\", return_tensors=\"pt\")\n",
    "output_logits = model(tokens.input_ids).logits\n",
    "\n",
    "print(output_logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c27988-e30d-4fb1-b4f1-d7fbfcab26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_decoding = output_logits.argmax(-1)\n",
    "tokenizer.decode(greedy_decoding[0, -1], skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0690d3e-9000-4efc-8067-8bd7a6479c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history  = tl.log_forward_pass(\n",
    "    model,\n",
    "    tokens.input_ids,\n",
    "    vis_opt=\"unrolled\",\n",
    "    vis_direction=\"topdown\",\n",
    "    vis_fileformat=\"svg\",\n",
    "    vis_nesting_depth=99,\n",
    "    vis_outpath=\"distilgpt2.svg\",\n",
    ")\n",
    "print(model_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90894193-956e-49af-a248-3e8437f0dc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}